wandb:
  project: VQTokenizer
data:
  pdb_dir: ./data
  patch_size: 8
  lmdb_dir: ./LMDB
  batch_size: 64
  num_workers: 4
model:
  hidden_dim: 1024
  latent_dim: 8
  nhead: 16
  learning_rate: 0.0005
  temperature: 1.0
  commitment_cost: 0.25
trainer:
  max_epochs: 50
  accelerator: auto
  checkpoint_dir: ./checkpoints
  checkpoint_name: lfqtokenizer-{epoch:02d}-{val_total_loss:.4f}
  save_top_k: 5
  monitor: val_total_loss
  monitor_mode: min
  log_every_n_steps: 100
  val_check_interval: 0.25
  final_model_path: ./weight/final_model.ckpt
  precision: BF16-mixed
  strategy: ddp
