wandb:
  project: VQTokenizer
data:
  pdb_dir: ./data
  patch_size: 8
  lmdb_dir: ./LMDB
  batch_size: 64
  num_workers: 4
model:
  hidden_dim: 1024
  latent_dim: 8
  nhead: 16
  learning_rate: 0.0005
  temperature: 1.0
  commitment_cost: 0.25
  lr_peak: 4e-4           # 峰值学习率
  lr_min: 4e-5            # 最小学习率
  lr_warmup_ratio: 0.02   # warmup步数占总步数的比例
  lr_decay_ratio: 0.9     # 衰减所占训练步数比例
  weight_decay: 0.01      # 权重衰减
trainer:
  max_epochs: 50
  accelerator: auto
  checkpoint_dir: ./checkpoints
  checkpoint_name: lfqtokenizer-{epoch:02d}-{val_total_loss:.4f}
  save_top_k: 5
  monitor: val_total_loss
  monitor_mode: min
  log_every_n_steps: 100
  val_check_interval: 0.25
  final_model_path: ./weight/final_model.ckpt
  precision: BF16-mixed
  strategy: ddp
